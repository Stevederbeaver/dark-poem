{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import gensim\n",
    "import random\n",
    "import string\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function for processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for processing each word (Capitalization and special characters)\n",
    "def parse_word(word, syllable_dict):\n",
    "    word = word.lower()\n",
    "    if word in syllable_dict:\n",
    "        return word\n",
    "    else:\n",
    "        special_character_list= [',', '.', '?', '!', ';', ':', '(', ')', \"'\"]\n",
    "        for j in range(2):\n",
    "            if word[-1:] in special_character_list:\n",
    "                word = word[:-1]\n",
    "                if word in syllable_dict:\n",
    "                    return word\n",
    "            if word[:1] in special_character_list:\n",
    "                word = word[1:]\n",
    "                if word in syllable_dict:\n",
    "                    return word\n",
    "        return word\n",
    "    \n",
    "# Helper function for reading the Syllable dictionary\n",
    "def get_syllable_dict(filename):\n",
    "    syllable_dict = {}\n",
    "    file = open(os.path.join(os.getcwd(), filename)).read().split('\\n')\n",
    "    \n",
    "    for line in file:\n",
    "        if line.split():\n",
    "            word_syllable = line.split()\n",
    "            real_count = []\n",
    "            end_count = []\n",
    "            # Deal with the ending cases\n",
    "            for i in range(1, len(word_syllable)):\n",
    "                if word_syllable[i][0] == 'E':\n",
    "                    end_count.append(int(word_syllable[i][1]))\n",
    "                else:\n",
    "                    real_count.append(int(word_syllable[i][0]))\n",
    "            # Rank the syllable from the highest to the lowest        \n",
    "            syllable_dict[word_syllable[0]] = [real_count[::-1], end_count[::-1]]\n",
    "\n",
    "    return syllable_dict\n",
    "\n",
    "# Find the syllable of an input word\n",
    "def find_syllable(word, syllable_dict, remain):\n",
    "        key = word.lower()\n",
    "        # Extract the real and end syllable lists\n",
    "        real_syllable = syllable_dict[key][0]\n",
    "        end_syllable = syllable_dict[key][1]\n",
    "        \n",
    "        # Check if the word's real syllable satisfies our requirement\n",
    "        for i in range(len(real_syllable)):\n",
    "            if real_syllable[i] <= remain:\n",
    "                return random.choice(real_syllable[i:])\n",
    "        \n",
    "        if len(end_syllable) != 0:\n",
    "            for j in range(len(end_syllable)):\n",
    "                if end_syllable[j] == remain:\n",
    "                    return end_syllable[j]\n",
    "        \n",
    "        # If there's no valid syllable within the range, return 11 > 10\n",
    "        return 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sentence length: 11\n"
     ]
    }
   ],
   "source": [
    "# Reading the file\n",
    "filename = 'data/shakespeare.txt'\n",
    "syllable_name = 'data/Syllable_dictionary.txt'\n",
    "Shakes_poem = open(os.path.join(os.getcwd(), filename)).read().split('\\n')\n",
    "\n",
    "syllable_dict = get_syllable_dict(syllable_name)\n",
    "sentences = []\n",
    "\n",
    "# Extracting observation and syllable_dict\n",
    "max_sentence_len = 0\n",
    "for sentence in Shakes_poem:\n",
    "    raw_list = sentence.split(' ')\n",
    "    if len(raw_list) != 1:\n",
    "        if raw_list[-1].isdigit():\n",
    "            continue\n",
    "        else:    \n",
    "            word_list = []\n",
    "            for word in raw_list:\n",
    "                if word != '':\n",
    "                    if word in syllable_dict:\n",
    "                        word_list.append(word)\n",
    "                    else:\n",
    "                        word_list.append(parse_word(word, syllable_dict))\n",
    "            sentences.append(word_list)\n",
    "\n",
    "for sentence in sentences:\n",
    "    new_length = len(sentence)\n",
    "    if new_length > max_sentence_len:\n",
    "        max_sentence_len = new_length\n",
    "print('Maximum sentence length:', max_sentence_len)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result embedding shape: (3205, 100)\n"
     ]
    }
   ],
   "source": [
    "word_model = gensim.models.Word2Vec(sentences, size=100, min_count=1, window=5, iter=100)\n",
    "pretrained_weights = word_model.wv.syn0\n",
    "vocab_size, embedding_size = pretrained_weights.shape\n",
    "print('Result embedding shape:', pretrained_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2idx(word):\n",
    "    return word_model.wv.vocab[word].index\n",
    "def idx2word(idx):\n",
    "    return word_model.wv.index2word[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing the data for LSTM...\n",
      "train_x shape: (2155, 11)\n",
      "train_y shape: (2155,)\n"
     ]
    }
   ],
   "source": [
    "print('\\nPreparing the data for LSTM...')\n",
    "X_train = np.zeros([len(sentences), max_sentence_len], dtype=np.int32)\n",
    "y_train = np.zeros([len(sentences)], dtype=np.int32)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, word in enumerate(sentence[:-1]):\n",
    "        X_train[i, t] = word2idx(word)\n",
    "    y_train[i] = word2idx(sentence[-1])\n",
    "print('train_x shape:', X_train.shape)\n",
    "print('train_y shape:', y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LSTM...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, None, 100)         320500    \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 3205)              323705    \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 3205)              0         \n",
      "=================================================================\n",
      "Total params: 724,605\n",
      "Trainable params: 724,605\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print('\\nTraining LSTM...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, weights=[pretrained_weights]))\n",
    "model.add(LSTM(units=embedding_size))\n",
    "model.add(Dense(units=vocab_size))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling function (combining softmax with temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating text with trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nFrom fairest creatures we desire increase,\\nThat thereby beauty's rose might never die,\\nBut as the riper should by time decease,\\nHis tender heir might bear his memory:\\nBut thou contracted to thine own bright eyes,\\nFeed'st thy light's flame with self-substantial fuel,\\nMaking a famine where abundance lies,\\nThy self thy foe, to thy sweet self too cruel:\\nThou that art now the world's fresh ornament,\\nAnd only herald to the gaudy spring,\\nWithin thine own bud buriest thy content,\\nAnd tender churl mak'st waste in niggarding:\\n  Pity the world, or else this glutton be,\\n  To eat the world's due, by the grave and thee.\\n\""
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Sample Poem\n",
    "'''\n",
    "From fairest creatures we desire increase,\n",
    "That thereby beauty's rose might never die,\n",
    "But as the riper should by time decease,\n",
    "His tender heir might bear his memory:\n",
    "But thou contracted to thine own bright eyes,\n",
    "Feed'st thy light's flame with self-substantial fuel,\n",
    "Making a famine where abundance lies,\n",
    "Thy self thy foe, to thy sweet self too cruel:\n",
    "Thou that art now the world's fresh ornament,\n",
    "And only herald to the gaudy spring,\n",
    "Within thine own bud buriest thy content,\n",
    "And tender churl mak'st waste in niggarding:\n",
    "  Pity the world, or else this glutton be,\n",
    "  To eat the world's due, by the grave and thee.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2155/2155 [==============================] - 2s 1ms/step - loss: 8.0051 - acc: 0.0139\n",
      "Epoch 2/30\n",
      "2155/2155 [==============================] - 1s 265us/step - loss: 7.0225 - acc: 0.0176\n",
      "Epoch 3/30\n",
      "2155/2155 [==============================] - 1s 242us/step - loss: 6.9830 - acc: 0.0181\n",
      "Epoch 4/30\n",
      "2155/2155 [==============================] - 0s 228us/step - loss: 6.9923 - acc: 0.0190\n",
      "Epoch 5/30\n",
      "2155/2155 [==============================] - 1s 244us/step - loss: 6.9905 - acc: 0.0116\n",
      "Epoch 6/30\n",
      "2155/2155 [==============================] - 1s 235us/step - loss: 6.9370 - acc: 0.0125\n",
      "Epoch 7/30\n",
      "2155/2155 [==============================] - 1s 257us/step - loss: 6.7852 - acc: 0.0167\n",
      "Epoch 8/30\n",
      "2155/2155 [==============================] - 1s 251us/step - loss: 6.6851 - acc: 0.0148\n",
      "Epoch 9/30\n",
      "2155/2155 [==============================] - 1s 247us/step - loss: 6.5020 - acc: 0.0223\n",
      "Epoch 10/30\n",
      "2155/2155 [==============================] - 1s 241us/step - loss: 6.1766 - acc: 0.0246\n",
      "Epoch 11/30\n",
      "2155/2155 [==============================] - 1s 245us/step - loss: 5.8036 - acc: 0.0422\n",
      "Epoch 12/30\n",
      "2155/2155 [==============================] - 1s 251us/step - loss: 5.3342 - acc: 0.0636\n",
      "Epoch 13/30\n",
      "2155/2155 [==============================] - 1s 254us/step - loss: 4.7870 - acc: 0.1012\n",
      "Epoch 14/30\n",
      "2155/2155 [==============================] - 1s 250us/step - loss: 4.1491 - acc: 0.1791\n",
      "Epoch 15/30\n",
      "2155/2155 [==============================] - 1s 240us/step - loss: 3.4619 - acc: 0.2910\n",
      "Epoch 16/30\n",
      "2155/2155 [==============================] - 1s 245us/step - loss: 2.7741 - acc: 0.4478\n",
      "Epoch 17/30\n",
      "2155/2155 [==============================] - 1s 241us/step - loss: 2.0700 - acc: 0.6297\n",
      "Epoch 18/30\n",
      "2155/2155 [==============================] - 1s 251us/step - loss: 1.4726 - acc: 0.7777\n",
      "Epoch 19/30\n",
      "2155/2155 [==============================] - 1s 238us/step - loss: 0.9903 - acc: 0.8794\n",
      "Epoch 20/30\n",
      "2155/2155 [==============================] - 1s 257us/step - loss: 0.6286 - acc: 0.9471 0s - loss: 0.5580 - acc: 0\n",
      "Epoch 21/30\n",
      "2155/2155 [==============================] - 1s 243us/step - loss: 0.3848 - acc: 0.9745\n",
      "Epoch 22/30\n",
      "2155/2155 [==============================] - 1s 251us/step - loss: 0.2288 - acc: 0.9935\n",
      "Epoch 23/30\n",
      "2155/2155 [==============================] - 1s 262us/step - loss: 0.1406 - acc: 0.9968\n",
      "Epoch 24/30\n",
      "2155/2155 [==============================] - 1s 258us/step - loss: 0.0862 - acc: 0.9991\n",
      "Epoch 25/30\n",
      "2155/2155 [==============================] - 1s 244us/step - loss: 0.0517 - acc: 0.9995\n",
      "Epoch 26/30\n",
      "2155/2155 [==============================] - 1s 250us/step - loss: 0.0254 - acc: 1.0000\n",
      "Epoch 27/30\n",
      "2155/2155 [==============================] - 1s 253us/step - loss: 0.0208 - acc: 0.9986\n",
      "Epoch 28/30\n",
      "2155/2155 [==============================] - 1s 268us/step - loss: 0.0538 - acc: 0.9981\n",
      "Epoch 29/30\n",
      "2155/2155 [==============================] - 1s 250us/step - loss: 0.0098 - acc: 1.0000\n",
      "Epoch 30/30\n",
      "2155/2155 [==============================] - 1s 247us/step - loss: 0.0038 - acc: 1.0000\n",
      "\n",
      "----- temperature parameter: 1.5\n",
      "From fairest sounds polished few \n",
      "That thereby suborned intermixed gluttoning \n",
      "But as neglected kiss eternity fond basest sin death \n",
      "His tender take graven arrest \n",
      "But thou prognosticate mad silent signs home benefit o'erworn \n",
      "Feed'st thy insufficiency compeers growth vexed gently speak centre \n",
      "Making a stretched alone sluttish sparkling stormy anchored \n",
      "Thy self quiet convert distills race soft ranged pursuing \n",
      "Thou that stained tongue cured hot astronomy forsworn ornaments \n",
      "And only lose men spend \n",
      "Within thine hung unless endeared break say homage \n",
      "And tender palate years young \n",
      "  Pity the moan colour others lived ow'st gate \n",
      "  To eat verse accumulate glory proves intermixed approve placed \n",
      "\n",
      "\n",
      "----- temperature parameter: 0.75\n",
      "From fairest prevailed hems nearly \n",
      "That thereby tell needing life \n",
      "But as those despair blame lies sing misuse me \n",
      "His tender bastard food offence \n",
      "But thou thought counterfeit ward grounded third reckon phrase \n",
      "Feed'st thy constancy grave light's sound all respect birth \n",
      "Making a imitate sadly viewest disdaineth desired accidents \n",
      "Thy self gems abhor forgot accumulate wardrobe mended arrest \n",
      "Thou that old now thrall stars forsworn heir skill \n",
      "And only farther fitted there \n",
      "Within thine chips souls new above stars aggravate \n",
      "And tender hours done compiled \n",
      "  Pity the yield debtor complain be compare upon \n",
      "  To eat forgot adonis plight shallowest face unlooked finds \n",
      "\n",
      "\n",
      "----- temperature parameter: 0.25\n",
      "From fairest hence feast king \n",
      "That thereby blindness anew thus \n",
      "But as assured will rehearse refusest mistaking proved is \n",
      "His tender breeds seek anew \n",
      "But thou away will arise resemble rehearse swearing assailed \n",
      "Feed'st thy constancy cries fall bright art there woman's \n",
      "Making a fear wrack kind posterity die adieu \n",
      "Thy self chips reproving true authority turned astronomy free \n",
      "Thou that forsworn me down free clear bright art \n",
      "And only proceeds free ear \n",
      "Within thine tear prevailed loss fate alone thereby \n",
      "And tender blame weeks intermixed \n",
      "  Pity the sad well-contented now dearly allayed rare \n",
      "  To eat be thee friend accumulate swearing me spies \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "M_syllable = 10\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=128, nb_epoch=30)\n",
    "    \n",
    "for temperature in [1.5, 0.75, 0.25]:\n",
    "    print()\n",
    "    print('----- temperature parameter:', temperature)\n",
    "        \n",
    "    poem = ''\n",
    "    capital_list = ['From fairest', 'That thereby', 'But as', 'His tender', 'But thou', \"Feed'st thy\", 'Making a',\n",
    "                         'Thy self', 'Thou that', 'And only', 'Within thine', 'And tender', 'Pity the', 'To eat']\n",
    "        \n",
    "    for i in range(14):\n",
    "        given_words = capital_list[i]\n",
    "        sentence = given_words + ' '\n",
    "        word_indexes = []\n",
    "        syllable_remain = M_syllable\n",
    "            \n",
    "        given_word_list = given_words.split(' ')\n",
    "        for word in given_word_list:\n",
    "            word = word.lower()\n",
    "            word_indexes.append(word2idx(word))\n",
    "            syllable_remain -= find_syllable(word, syllable_dict, M_syllable)\n",
    "            \n",
    "        while syllable_remain > 1:\n",
    "            prediction = model.predict(x=np.array(word_indexes), verbose=0)\n",
    "            next_index = sample(prediction[-1], temperature)\n",
    "            next_word = idx2word(next_index)\n",
    "            next_syllable = find_syllable(word, syllable_dict, syllable_remain)\n",
    "                \n",
    "            if next_syllable != 11:\n",
    "                syllable_remain -= next_syllable\n",
    "                word_indexes.append(next_index)\n",
    "            \n",
    "        for j in range(len(word_indexes) - 2):\n",
    "            sentence += idx2word(word_indexes[j + 2]) + ' '\n",
    "    \n",
    "        if i == 12 or i == 13:\n",
    "            sentence = '  ' + sentence\n",
    "            \n",
    "        poem += sentence + '\\n'\n",
    "        \n",
    "    print(poem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
