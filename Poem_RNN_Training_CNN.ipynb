{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM, Conv1D, MaxPooling1D, Flatten, BatchNormalization\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "\n",
    "from functools import partial\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of poems: 154\n"
     ]
    }
   ],
   "source": [
    "text = open(os.path.join(os.getcwd(), 'data/shakespeare.txt')).read().lower().split('\\n')\n",
    "poem_list = []\n",
    "raw_text = ''\n",
    "for j in range(len(text) + 1):\n",
    "    if j == len(text):\n",
    "        poem_list.append(raw_text)\n",
    "    elif text[j] == '':\n",
    "        if raw_text != '':\n",
    "            poem_list.append(raw_text)\n",
    "        raw_text = ''\n",
    "        continue\n",
    "    elif text[j][-1].isdigit():\n",
    "        continue\n",
    "    else:\n",
    "        subsentence = text[j] + '\\n'\n",
    "        raw_text += subsentence  \n",
    "print('Number of poems:', len(poem_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize into sequences of characters\n",
    "def create_sequence(raw_text, length, step):\n",
    "    sequences = []\n",
    "    next_chars = []\n",
    "    for i in range(0, len(raw_text) - length, step):\n",
    "        # select sequence of tokens\n",
    "        seq = raw_text[i:i + length]\n",
    "        # store\n",
    "        sequences.append(seq)\n",
    "        next_chars.append(raw_text[i + length])\n",
    "    return sequences, next_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 88130\n"
     ]
    }
   ],
   "source": [
    "length = 40\n",
    "step = 1\n",
    "sequences = []\n",
    "next_chars = []\n",
    "for poem in poem_list:\n",
    "    sub_sequences, sub_next_chars = create_sequence(poem, length, step)\n",
    "    sequences += sub_sequences\n",
    "    next_chars += sub_next_chars\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Mappings and Inverse Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 38\n"
     ]
    }
   ],
   "source": [
    "poem_string = \"\".join(poem_list)\n",
    "chars = sorted(list(set(poem_string)))\n",
    "char_index_map = dict((c, i) for i, c in enumerate(chars))\n",
    "index_char_map = dict((i, c) for i, c in enumerate(chars))\n",
    "vocab_size = len(char_index_map)\n",
    "print('Vocabulary Size: %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(sequences), length, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sequences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_index_map[char]] = 1\n",
    "    y[i, char_index_map[next_chars[i]]] = 1    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the RNN Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "WARNING:tensorflow:From C:\\Users\\Steve\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Steve\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Steve\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Steve\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Steve\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 36, 64)            12224     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 18, 64)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 18, 64)            256       \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 38)                4902      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 38)                0         \n",
      "=================================================================\n",
      "Total params: 116,198\n",
      "Trainable params: 116,070\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\Steve\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Steve\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters = 64,\n",
    "                 kernel_size = 5,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1, input_shape=(length, len(chars))))\n",
    "model.add(MaxPooling1D(pool_size = 2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()   \n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling function (combining softmax with temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model and generating outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Steve\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\Steve\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Steve\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Steve\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\Steve\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Steve\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Steve\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Steve\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Steve\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "88130/88130 [==============================] - 96s 1ms/step - loss: 1.9779 - acc: 0.4193\n",
      "Epoch 2/10\n",
      "88130/88130 [==============================] - 101s 1ms/step - loss: 1.7173 - acc: 0.4844\n",
      "Epoch 3/10\n",
      "88130/88130 [==============================] - 102s 1ms/step - loss: 1.6412 - acc: 0.5039\n",
      "Epoch 4/10\n",
      "88130/88130 [==============================] - 104s 1ms/step - loss: 1.5907 - acc: 0.5154\n",
      "Epoch 5/10\n",
      "88130/88130 [==============================] - 100s 1ms/step - loss: 1.5532 - acc: 0.5265\n",
      "Epoch 6/10\n",
      "88130/88130 [==============================] - ETA: 0s - loss: 1.5278 - acc: 0.532 - 100s 1ms/step - loss: 1.5278 - acc: 0.5322\n",
      "Epoch 7/10\n",
      "88130/88130 [==============================] - 109s 1ms/step - loss: 1.5078 - acc: 0.5362\n",
      "Epoch 8/10\n",
      "88130/88130 [==============================] - 113s 1ms/step - loss: 1.4908 - acc: 0.5424\n",
      "Epoch 9/10\n",
      "88130/88130 [==============================] - 105s 1ms/step - loss: 1.4690 - acc: 0.5458\n",
      "Epoch 10/10\n",
      "88130/88130 [==============================] - 109s 1ms/step - loss: 1.4560 - acc: 0.5494\n",
      "\n",
      "----- temperature parameter: 1.5\n",
      "----- Generating with seed: \"shall i compare thee to a summer's day?\n",
      "\"\n",
      "shall i compare thee to a summer's day?\n",
      "that wighthe, sablenf,\n",
      "it wheree weemn hastears' creadnate, fore,\n",
      "sdead mouls feath, less act,\n",
      "insuch up were than it vertast plgwling,\n",
      "a grantenf we nulsy what when thee true,\n",
      "ever, never-poesttay love\n",
      "thesit dusbutte grulong glory bais thath io,\n",
      "my derfwent my bread seek,\n",
      "but pone weight,\n",
      "wad methounds-incuve world-ug:\n",
      "for ye lsas you mases,\n",
      "ly id.\n",
      "that floloss mofave socghvaem, if strength,\n",
      "\n",
      "\n",
      "----- temperature parameter: 0.75\n",
      "----- Generating with seed: \"shall i compare thee to a summer's day?\n",
      "\"\n",
      "shall i compare thee to a summer's day?\n",
      "the fairer grord stranger the steep'st being thee,\n",
      "which hence that not yet your self (beauty. thou dost part,\n",
      "that me not thy self blunt then theren to whence.\n",
      "  my break of this profase decorn thee,\n",
      "that with liking with shall my self?\n",
      "than all not and my burthen shall this will,\n",
      "and living and thy beauty errlk of mark kong,\n",
      "but with being that doth self your eye all not,\n",
      "so leaves escerse the flack,\n",
      "when i hear not and their brave that i praise,\n",
      "that mine eyes hast my self-your stell,\n",
      "yet doth seem theich day mend eyesw transfront not this,\n",
      "the all as shall for i have posters,\n",
      "\n",
      "\n",
      "----- temperature parameter: 0.25\n",
      "----- Generating with seed: \"shall i compare thee to a summer's day?\n",
      "\"\n",
      "shall i compare thee to a summer's day?\n",
      "then that beauty of thy self the beauty,\n",
      "that bear from the world that the world,\n",
      "that beauty my self that for thee thee,\n",
      "  then that those that he wlend the that be is self with thee,\n",
      "  and beauty of thy self the faires, and then thee,\n",
      "  then thee that be for the state the such thee,\n",
      "and must shall the beauty of the state thee,\n",
      "and there beauty state the beauty me so.\n",
      "  for the state the state of the state thee,\n",
      "and there was shall the world that then thee,\n",
      "that become which that which be the surmers thee,\n",
      "and then thee that that i that i have so,\n",
      "that i have seem seem the true that be.\n",
      "\n",
      "Epoch 1/10\n",
      "88130/88130 [==============================] - 112s 1ms/step - loss: 1.4512 - acc: 0.5510\n",
      "Epoch 2/10\n",
      "88130/88130 [==============================] - 110s 1ms/step - loss: 1.4415 - acc: 0.5537\n",
      "Epoch 3/10\n",
      "88130/88130 [==============================] - 119s 1ms/step - loss: 1.4368 - acc: 0.5559\n",
      "Epoch 4/10\n",
      "88130/88130 [==============================] - 118s 1ms/step - loss: 1.4273 - acc: 0.5576\n",
      "Epoch 5/10\n",
      "88130/88130 [==============================] - 128s 1ms/step - loss: 1.4259 - acc: 0.5589\n",
      "Epoch 6/10\n",
      "88130/88130 [==============================] - 128s 1ms/step - loss: 1.4257 - acc: 0.5588\n",
      "Epoch 7/10\n",
      "88130/88130 [==============================] - 131s 1ms/step - loss: 1.4270 - acc: 0.5583\n",
      "Epoch 8/10\n",
      "88130/88130 [==============================] - 134s 2ms/step - loss: 1.4276 - acc: 0.5584\n",
      "Epoch 9/10\n",
      "88130/88130 [==============================] - 137s 2ms/step - loss: 1.4272 - acc: 0.5572\n",
      "Epoch 10/10\n",
      "88130/88130 [==============================] - 139s 2ms/step - loss: 1.4356 - acc: 0.5555\n",
      "\n",
      "----- temperature parameter: 1.5\n",
      "----- Generating with seed: \"shall i compare thee to a summer's day?\n",
      "\"\n",
      "shall i compare thee to a summer's day?\n",
      "heir dud)e of you by heart i nemasuviet,\n",
      "to griting thou long is heel, looksiss\n",
      "  dummer time's caumus is, do wouldst repues:d\n",
      "on do noot's lucyoure. jurpefev in freeql i commits,\n",
      "odund reason thee contau swing till. so give.\n",
      "lot trat i am clasceding still i'lliec:\n",
      "so thy chilireinedst i hye's nose hunss nrey time,\n",
      "for deinta howicess no crowfhour phow\n",
      "cojty?al faerinv,v meveru a(ocmowring:\n",
      "\n",
      "ha nather rue nous,\n",
      "when hislestss upesr eyes mores i lose,\n",
      "not ten taughnt'ss caswerdsoscedd, if pity set's jint,\n",
      "\n",
      "\n",
      "----- temperature parameter: 0.75\n",
      "----- Generating with seed: \"shall i compare thee to a summer's day?\n",
      "\"\n",
      "shall i compare thee to a summer's day?\n",
      "nor thy love loss best it affectang things evergut,\n",
      "and you new-false i manst i am some share.\n",
      "  me how seem in my painted every hatel,\n",
      "more in wilt pul i mill he thee in more,\n",
      "cannting is him owing world with thee i new.\n",
      "leses of enourn of his mine accers, but they,\n",
      "his prime of the on state course in wind\n",
      "wornols have i none in parts to mind,\n",
      "and iel yet me eyes, and dumb looks,\n",
      "and her even should can as out that ranger and,\n",
      "sin' on the bases an my least the tongues,\n",
      "on have where he couls, my heart hours she in mine own veect,\n",
      "the others to the calse (us becoming,\n",
      "\n",
      "\n",
      "----- temperature parameter: 0.25\n",
      "----- Generating with seed: \"shall i compare thee to a summer's day?\n",
      "\"\n",
      "shall i compare thee to a summer's day?\n",
      "the self in the world in the me the me,\n",
      "  and to the world the old and the world,\n",
      "and the first the confuned the sound thee,\n",
      "the world the thee as to me, the world thee,\n",
      "and the sound the the see my self all the world,\n",
      "when the sound strangey that the world deceis,\n",
      "the world thee in the the world thee founter eyes,\n",
      "and seeming as the world the world me issed,\n",
      "and the conruse, and to thee beauty, and there,\n",
      "and to the more to the tombed shalt form me could,\n",
      "and the bases and the sound no summer be.\n",
      "  the world the see as out of this in me,\n",
      "  and in the summer sightly the true their stand\n",
      "\n",
      "Epoch 1/10\n",
      "88130/88130 [==============================] - 138s 2ms/step - loss: 1.4355 - acc: 0.5572\n",
      "Epoch 2/10\n",
      "88130/88130 [==============================] - 146s 2ms/step - loss: 1.4380 - acc: 0.5544\n",
      "Epoch 3/10\n",
      "88130/88130 [==============================] - 147s 2ms/step - loss: 1.4433 - acc: 0.5562\n",
      "Epoch 4/10\n",
      "88130/88130 [==============================] - 152s 2ms/step - loss: 1.4416 - acc: 0.5566\n",
      "Epoch 5/10\n",
      "88130/88130 [==============================] - 152s 2ms/step - loss: 1.4531 - acc: 0.5540\n",
      "Epoch 6/10\n",
      "88130/88130 [==============================] - 154s 2ms/step - loss: 1.4553 - acc: 0.5532\n",
      "Epoch 7/10\n",
      "88130/88130 [==============================] - 158s 2ms/step - loss: 1.4599 - acc: 0.5511\n",
      "Epoch 8/10\n",
      "88130/88130 [==============================] - 160s 2ms/step - loss: 1.4631 - acc: 0.5508\n",
      "Epoch 9/10\n",
      "88130/88130 [==============================] - 163s 2ms/step - loss: 1.4672 - acc: 0.5504\n",
      "Epoch 10/10\n",
      "88130/88130 [==============================] - 170s 2ms/step - loss: 1.4678 - acc: 0.5506\n",
      "\n",
      "----- temperature parameter: 1.5\n",
      "----- Generating with seed: \"shall i compare thee to a summer's day?\n",
      "\"\n",
      "shall i compare thee to a summer's day?\n",
      "  kta men judgecaleogsty ruys what my mind,\n",
      "  wo giveige,\n",
      " my tlove and being she takety a rredson inwifchie!\n",
      "o good of as effose, wookep bressinge naw,\n",
      "rowis farsee spotled falfeeds tan wulfig by,\n",
      "thereto slony in my tell comfleecion, and loscald,\n",
      "hence,s will ocunhze usil be thiglsasx\n",
      "car bellingl cuicued tickle blunty would\n",
      "  in as jests chantifumssew-hy sweeting anetty to greech,\n",
      "gramen liembcubrfec rarisf is is pensemfar.\n",
      "not scade selfoes gibof no-diy, lovy nrine,\n",
      "yet im teith, somwy'wsahs detorligon moalse,\n",
      "but u priwfensuch's  orvevrinoer woeg's wrearul age it,\n",
      "\n",
      "\n",
      "----- temperature parameter: 0.75\n",
      "----- Generating with seed: \"shall i compare thee to a summer's day?\n",
      "\"\n",
      "shall i compare thee to a summer's day?\n",
      "when bagk of my self the thee, the cease,\n",
      "the bonder some unsey,\n",
      "and de it features you are of love lousing,\n",
      "for thee, a  seaph thou and with so sing, where of is doth inu,\n",
      "wherein thou paintapt to giveous,\n",
      "that detay, look with my thou the foile bude\n",
      "which is for your for you\n",
      "?bpy dof. coure did for thee as pine\n",
      "windom i to then be mort muse, but thou they so nebply ectoad do im rey,\n",
      "which morey to loving of say it to meece.\n",
      "  ow thee better with self makes of my fair,\n",
      "whose breathe dear for my thus, like a dost by their wide,\n",
      "i see of my breating in needs of my marked time\n",
      "\n",
      "\n",
      "----- temperature parameter: 0.25\n",
      "----- Generating with seed: \"shall i compare thee to a summer's day?\n",
      "\"\n",
      "shall i compare thee to a summer's day?\n",
      "thou dost thou some do in the subme when with thou memory,\n",
      "and the worse thou dost thou art beauty's store,\n",
      "  and so found the worst my self a turned shows then my belong,\n",
      "the world thou art beauty the world my self i thou are thee.\n",
      "  so thou art the world thou art beauty's sute thee,\n",
      "  and the with the worst thou dost thou stranged the summon sickee,\n",
      "the souls the sweet so thou gaves an my shall seem thou grow,\n",
      "to the world the thee thou art my self thee.\n",
      "  so i to the world thee my self and sease,\n",
      "to the suppey thou art my self the worse better that be, where is waster thee,\n",
      "  the self thou still shall the summer's doth thou love's grow:\n",
      "  so thou dost thou art the with sinks thee,\n",
      "and the worst thou art shall be the seaar,\n",
      "\n",
      "Epoch 1/10\n",
      "88130/88130 [==============================] - 150s 2ms/step - loss: 1.4712 - acc: 0.5499\n",
      "Epoch 2/10\n",
      "88130/88130 [==============================] - 161s 2ms/step - loss: 1.4827 - acc: 0.5477\n",
      "Epoch 3/10\n",
      "88130/88130 [==============================] - 179s 2ms/step - loss: 1.4849 - acc: 0.5464\n",
      "Epoch 4/10\n",
      "88130/88130 [==============================] - 183s 2ms/step - loss: 1.5051 - acc: 0.5421\n",
      "Epoch 5/10\n",
      "88130/88130 [==============================] - 184s 2ms/step - loss: 1.5119 - acc: 0.5396\n",
      "Epoch 6/10\n",
      "88130/88130 [==============================] - 190s 2ms/step - loss: 1.5139 - acc: 0.5411\n",
      "Epoch 7/10\n",
      "88130/88130 [==============================] - 190s 2ms/step - loss: 1.5169 - acc: 0.5380\n",
      "Epoch 8/10\n",
      "88130/88130 [==============================] - 197s 2ms/step - loss: 1.5187 - acc: 0.5380\n",
      "Epoch 9/10\n",
      "88130/88130 [==============================] - 201s 2ms/step - loss: 1.5235 - acc: 0.5389\n",
      "Epoch 10/10\n",
      "88130/88130 [==============================] - 202s 2ms/step - loss: 1.5142 - acc: 0.5393\n",
      "\n",
      "----- temperature parameter: 1.5\n",
      "----- Generating with seed: \"shall i compare thee to a summer's day?\n",
      "\"\n",
      "shall i compare thee to a summer's day?\n",
      "again? his voryn to it:e\n",
      "quesh oriactual gone\n",
      " tauth dy tle new,\n",
      "i living it bittle new dayies come freshand py.\n",
      "ul)ing, thou fretain his cruec to icaccest ix sweeracticg,\n",
      "cre mainty of illt canstiend waith bost:\n",
      "prorves swifh found i afmy:\n",
      "so sometimednes from motcathoudein,\n",
      "eye, which pite forming nyirg or sucholdy.\n",
      "low which betwreuw too my blooddd pride,\n",
      "time wonding beauty our thilano that deteon,\n",
      "sepieb but are as curs discaed to within teadure; in,\n",
      "if mis pice in,e more infor wanes fore halmy\n",
      "\n",
      "\n",
      "----- temperature parameter: 0.75\n",
      "----- Generating with seed: \"shall i compare thee to a summer's day?\n",
      "\"\n",
      "shall i compare thee to a summer's day?\n",
      "whil accinc thas that thou my fair thral weess?\n",
      "trul beauty dother be love on the world.\n",
      "the blight all the comments and good me,\n",
      "  and party not me than of you,\n",
      "or is subt heaven my love gause not fire,\n",
      "what the mained my love in thee breascet:\n",
      "that thou conlopedy, that and the mind in wit:\n",
      "and i that this evein my glass thou goldend,\n",
      "which prouds, where of goling all beauteins with weal for grows,\n",
      "what this prossings him ait-by and putlede,\n",
      "or thanst, in in prove in my youre word,\n",
      "the cares to this sub'er tare plessing,\n",
      "and lines accests at me worse worls your name:\n",
      "\n",
      "\n",
      "----- temperature parameter: 0.25\n",
      "----- Generating with seed: \"shall i compare thee to a summer's day?\n",
      "\"\n",
      "shall i compare thee to a summer's day?\n",
      "the with that that thou minute as the beauty,\n",
      "and see the blood thou mends my self than my that the seeming,\n",
      "the world the mind of my that the beauty,\n",
      "and that thou art in the breathe than what the great me,\n",
      "  and that being me that and that beauty,\n",
      "the world in the this thou may heart, and the subees the great thend,\n",
      "and the beauty the beauty so shall the sube thee,\n",
      "  and that bound that beauty shame shall my sofce that the seeps more that see,\n",
      "  and that brow the from the state as thou shall the dead, and the grace,\n",
      "and that in the mind of the thou stand'st so doth dear,\n",
      "that me that thou art wreteon the me the mead,\n",
      "and that thou art thou art the speed in thee,\n",
      "  and seeming the book to the speak of the mind,\n",
      "\n",
      "Epoch 1/10\n",
      "88130/88130 [==============================] - 213s 2ms/step - loss: 1.5282 - acc: 0.5355\n",
      "Epoch 2/10\n",
      "83456/88130 [===========================>..] - ETA: 12s - loss: 1.5295 - acc: 0.5387"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for iteration in range(5):\n",
    "    model.fit(X, y, batch_size=128, epochs=10)\n",
    "\n",
    "    for temperature in [1.5, 0.75, 0.25]:\n",
    "        print()\n",
    "        print('----- temperature parameter:', temperature)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = \"shall i compare thee to a summer's day?\\n\"\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        line_count = 0\n",
    "        while (line_count <= 12):\n",
    "            x = np.zeros((1, length, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x[0, t, char_index_map[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_char = index_char_map[next_index]\n",
    "            \n",
    "            if next_char == '\\n':\n",
    "                line_count += 1\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
