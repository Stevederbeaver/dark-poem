{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM, Conv1D, MaxPooling1D, Flatten, BatchNormalization\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "\n",
    "from functools import partial\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of poems: 154\n"
     ]
    }
   ],
   "source": [
    "text = open(os.path.join(os.getcwd(), 'data/shakespeare.txt')).read().lower().split('\\n')\n",
    "poem_list = []\n",
    "raw_text = ''\n",
    "for j in range(len(text) + 1):\n",
    "    if j == len(text):\n",
    "        poem_list.append(raw_text)\n",
    "    elif text[j] == '':\n",
    "        if raw_text != '':\n",
    "            poem_list.append(raw_text)\n",
    "        raw_text = ''\n",
    "        continue\n",
    "    elif text[j][-1].isdigit():\n",
    "        continue\n",
    "    else:\n",
    "        subsentence = text[j] + '\\n'\n",
    "        raw_text += subsentence  \n",
    "print('Number of poems:', len(poem_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize into sequences of characters\n",
    "def create_sequence(raw_text, length, step):\n",
    "    sequences = []\n",
    "    next_chars = []\n",
    "    for i in range(0, len(raw_text) - length, step):\n",
    "        # select sequence of tokens\n",
    "        seq = raw_text[i:i + length]\n",
    "        # store\n",
    "        sequences.append(seq)\n",
    "        next_chars.append(raw_text[i + length])\n",
    "    return sequences, next_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 88130\n"
     ]
    }
   ],
   "source": [
    "length = 40\n",
    "step = 1\n",
    "sequences = []\n",
    "next_chars = []\n",
    "for poem in poem_list:\n",
    "    sub_sequences, sub_next_chars = create_sequence(poem, length, step)\n",
    "    sequences += sub_sequences\n",
    "    next_chars += sub_next_chars\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Mappings and Inverse Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 38\n"
     ]
    }
   ],
   "source": [
    "poem_string = \"\".join(poem_list)\n",
    "chars = sorted(list(set(poem_string)))\n",
    "char_index_map = dict((c, i) for i, c in enumerate(chars))\n",
    "index_char_map = dict((i, c) for i, c in enumerate(chars))\n",
    "vocab_size = len(char_index_map)\n",
    "print('Vocabulary Size: %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(sequences), length, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sequences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_index_map[char]] = 1\n",
    "    y[i, char_index_map[next_chars[i]]] = 1    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the RNN Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "WARNING:tensorflow:From C:\\Users\\happy\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\happy\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\happy\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\happy\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\happy\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 36, 64)            12224     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 18, 64)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 18, 64)            256       \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 38)                4902      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 38)                0         \n",
      "=================================================================\n",
      "Total params: 116,198\n",
      "Trainable params: 116,070\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\happy\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\happy\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters = 64,\n",
    "                 kernel_size = 5,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1, input_shape=(length, len(chars))))\n",
    "model.add(MaxPooling1D(pool_size = 2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()   \n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling function (combining softmax with temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model and generating outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "88130/88130 [==============================] - 51s 573us/step - loss: 1.7970 - acc: 0.4644\n",
      "Epoch 2/50\n",
      "88130/88130 [==============================] - 33s 369us/step - loss: 1.6771 - acc: 0.4935\n",
      "Epoch 3/50\n",
      "88130/88130 [==============================] - 43s 484us/step - loss: 1.6178 - acc: 0.5080\n",
      "Epoch 4/50\n",
      "88130/88130 [==============================] - 61s 694us/step - loss: 1.5742 - acc: 0.5187\n",
      "Epoch 5/50\n",
      "88130/88130 [==============================] - 55s 624us/step - loss: 1.5492 - acc: 0.5241\n",
      "Epoch 6/50\n",
      "88130/88130 [==============================] - 56s 635us/step - loss: 1.5205 - acc: 0.5332\n",
      "Epoch 7/50\n",
      "88130/88130 [==============================] - 61s 696us/step - loss: 1.5013 - acc: 0.5371\n",
      "Epoch 8/50\n",
      "88130/88130 [==============================] - 61s 690us/step - loss: 1.4870 - acc: 0.5431\n",
      "Epoch 9/50\n",
      "88130/88130 [==============================] - 46s 519us/step - loss: 1.4697 - acc: 0.5464\n",
      "Epoch 10/50\n",
      "88130/88130 [==============================] - 51s 574us/step - loss: 1.4608 - acc: 0.54881s - \n",
      "Epoch 11/50\n",
      "88130/88130 [==============================] - 46s 527us/step - loss: 1.4532 - acc: 0.5496\n",
      "Epoch 12/50\n",
      "88130/88130 [==============================] - 48s 544us/step - loss: 1.4473 - acc: 0.5517\n",
      "Epoch 13/50\n",
      "88130/88130 [==============================] - 51s 583us/step - loss: 1.4431 - acc: 0.5526\n",
      "Epoch 14/50\n",
      "88130/88130 [==============================] - 54s 615us/step - loss: 1.4380 - acc: 0.5540\n",
      "Epoch 15/50\n",
      "88130/88130 [==============================] - 59s 672us/step - loss: 1.4388 - acc: 0.5534\n",
      "Epoch 16/50\n",
      "88130/88130 [==============================] - 60s 680us/step - loss: 1.4402 - acc: 0.5535\n",
      "Epoch 17/50\n",
      "88130/88130 [==============================] - 64s 730us/step - loss: 1.4389 - acc: 0.55206s - loss: 1.4326 - acc - ETA: 5s - ETA: 3s - l - ETA: 0s - loss: 1.4381 - a\n",
      "Epoch 18/50\n",
      "88130/88130 [==============================] - 69s 779us/step - loss: 1.4424 - acc: 0.5545\n",
      "Epoch 19/50\n",
      "88130/88130 [==============================] - 72s 821us/step - loss: 1.4461 - acc: 0.5529\n",
      "Epoch 20/50\n",
      "88130/88130 [==============================] - 75s 853us/step - loss: 1.4475 - acc: 0.5515\n",
      "Epoch 21/50\n",
      "88130/88130 [==============================] - 78s 886us/step - loss: 1.4551 - acc: 0.5500\n",
      "Epoch 22/50\n",
      "88130/88130 [==============================] - 80s 910us/step - loss: 1.4483 - acc: 0.5510\n",
      "Epoch 23/50\n",
      "88130/88130 [==============================] - 83s 938us/step - loss: 1.4501 - acc: 0.5537\n",
      "Epoch 24/50\n",
      "88130/88130 [==============================] - 86s 971us/step - loss: 1.4599 - acc: 0.5492\n",
      "Epoch 25/50\n",
      "88130/88130 [==============================] - 89s 1ms/step - loss: 1.4617 - acc: 0.5495\n",
      "Epoch 26/50\n",
      "88130/88130 [==============================] - 92s 1ms/step - loss: 1.4742 - acc: 0.5475\n",
      "Epoch 27/50\n",
      "88130/88130 [==============================] - 95s 1ms/step - loss: 1.4748 - acc: 0.5486\n",
      "Epoch 28/50\n",
      "88130/88130 [==============================] - 99s 1ms/step - loss: 1.4760 - acc: 0.5477\n",
      "Epoch 29/50\n",
      "88130/88130 [==============================] - 101s 1ms/step - loss: 1.4866 - acc: 0.5422\n",
      "Epoch 30/50\n",
      "88130/88130 [==============================] - 107s 1ms/step - loss: 1.4939 - acc: 0.5429\n",
      "Epoch 31/50\n",
      "88130/88130 [==============================] - 107s 1ms/step - loss: 1.5019 - acc: 0.5418\n",
      "Epoch 32/50\n",
      "88130/88130 [==============================] - 111s 1ms/step - loss: 1.5027 - acc: 0.5407\n",
      "Epoch 33/50\n",
      "88130/88130 [==============================] - 115s 1ms/step - loss: 1.5082 - acc: 0.5392\n",
      "Epoch 34/50\n",
      "88130/88130 [==============================] - 118s 1ms/step - loss: 1.5045 - acc: 0.5412\n",
      "Epoch 35/50\n",
      "88130/88130 [==============================] - 123s 1ms/step - loss: 1.5203 - acc: 0.5377 0s - loss: 1.5202 - acc: 0.537\n",
      "Epoch 36/50\n",
      "88130/88130 [==============================] - 128s 1ms/step - loss: 1.5202 - acc: 0.5355\n",
      "Epoch 37/50\n",
      "88130/88130 [==============================] - 131s 1ms/step - loss: 1.5253 - acc: 0.5362 12s - los - ETA: 2s - loss: 1.5238 \n",
      "Epoch 38/50\n",
      "88130/88130 [==============================] - 133s 2ms/step - loss: 1.5366 - acc: 0.5331A: 2:04 - loss: - ET - ETA: 0s - loss: 1.5361 - acc: 0\n",
      "Epoch 39/50\n",
      "88130/88130 [==============================] - 136s 2ms/step - loss: 1.5311 - acc: 0.5348\n",
      "Epoch 40/50\n",
      "88130/88130 [==============================] - 139s 2ms/step - loss: 1.5399 - acc: 0.5338\n",
      "Epoch 41/50\n",
      "88130/88130 [==============================] - 144s 2ms/step - loss: 1.5524 - acc: 0.5317\n",
      "Epoch 42/50\n",
      "88130/88130 [==============================] - 146s 2ms/step - loss: 1.5625 - acc: 0.5285\n",
      "Epoch 43/50\n",
      "88130/88130 [==============================] - 149s 2ms/step - loss: 1.5628 - acc: 0.5263\n",
      "Epoch 44/50\n",
      "88130/88130 [==============================] - 153s 2ms/step - loss: 1.5592 - acc: 0.5289\n",
      "Epoch 45/50\n",
      "88130/88130 [==============================] - 156s 2ms/step - loss: 1.5581 - acc: 0.5290\n",
      "Epoch 46/50\n",
      "88130/88130 [==============================] - 161s 2ms/step - loss: 1.5783 - acc: 0.5234\n",
      "Epoch 47/50\n",
      "88130/88130 [==============================] - 165s 2ms/step - loss: 1.5790 - acc: 0.5235\n",
      "Epoch 48/50\n",
      "88130/88130 [==============================] - 167s 2ms/step - loss: 1.5707 - acc: 0.5254\n",
      "Epoch 49/50\n",
      "88130/88130 [==============================] - 169s 2ms/step - loss: 1.5808 - acc: 0.5243\n",
      "Epoch 50/50\n",
      "88130/88130 [==============================] - 173s 2ms/step - loss: 1.5876 - acc: 0.5220\n",
      "\n",
      "----- temperature parameter: 1.5\n",
      "----- Generating with seed: \"shall i compare thee to a summer's day?\n",
      "\"\n",
      "shall i compare thee to a summer's day?\n",
      "disired doth loten night afterwired?\n",
      "  loot is a full eyed, ors apvinved gaare.\n",
      "when thrust altiming eare dwelll's opers am ofunliwo,\n",
      "be mistage weere's steetlciore flowers comose,\n",
      "or aletk knows thee brasted heoflery unoth.\n",
      "o thanrany, it mogchs sways by tonel.\n",
      "and give anture motmeint hims of fers placrine's,\n",
      "alonging not tack goes threwn hath far time)\n",
      "not rered jewel prisont leave to avermf aummer\n",
      "edtr'i hrow nane sor five's ead,\n",
      "using gained anule conben noom she curi,\n",
      "numing time sliaves in stees whit inwaliarly,\n",
      "deov'nsetfcl my e'ey eyide with in thee,\n",
      "\n",
      "\n",
      "----- temperature parameter: 0.75\n",
      "----- Generating with seed: \"shall i compare thee to a summer's day?\n",
      "\"\n",
      "shall i compare thee to a summer's day?\n",
      "  wild far to steal at be no blering beaut.\n",
      "  but to me a woettuen dwill, and so founts\n",
      "sumaect the might and again to less in that is shade,\n",
      "the that lawcing with on some deade mine,\n",
      "but shall to be are even my gone,\n",
      "and by yourse is is sad pity,\n",
      "my heart truths shadow my fair faili aj in do brojenf for besiker,\n",
      "a joutiin thy hath in his his mad, staig,\n",
      "being the would worths stop at not my self,\n",
      "  and starign too all should bart the bean,\n",
      "and therefore gracinaeck of end, own with eecw part.\n",
      "to praise your do and a lome's and gracit,\n",
      "and then age' de lies in the for thee but row,\n",
      "\n",
      "\n",
      "----- temperature parameter: 0.25\n",
      "----- Generating with seed: \"shall i compare thee to a summer's day?\n",
      "\"\n",
      "shall i compare thee to a summer's day?\n",
      "and this not the seemed and the beauty of thee,\n",
      "and thou my self this confeered to me,\n",
      "to me a loss thou my all to parts and thought,\n",
      "and this in thee art my thee to the seemed:\n",
      "then i shame hath with the seeming thy seee wind,\n",
      "and thou my self shall to parts, and and of thee,\n",
      "and and my self thou art stand to my self,\n",
      "and thou my self away, and stand to,\n",
      "that i sea it me thee to the worth to be than my love,\n",
      "  then my love and thou my love and thee,\n",
      "and thou my self though the seemen to steal to cannot to thee,\n",
      "that i am shall in thee to see are to with a come,\n",
      "and and thou my love and that to show,\n",
      "\n",
      "Epoch 1/50\n",
      "88130/88130 [==============================] - 182s 2ms/step - loss: 1.5873 - acc: 0.5232\n",
      "Epoch 2/50\n",
      "88130/88130 [==============================] - 182s 2ms/step - loss: 1.6046 - acc: 0.5199\n",
      "Epoch 3/50\n",
      "88130/88130 [==============================] - 185s 2ms/step - loss: 1.5932 - acc: 0.5201\n",
      "Epoch 4/50\n",
      "88130/88130 [==============================] - 189s 2ms/step - loss: 1.5924 - acc: 0.5231\n",
      "Epoch 5/50\n",
      "88130/88130 [==============================] - 190s 2ms/step - loss: 1.6006 - acc: 0.5200\n",
      "Epoch 6/50\n",
      "69248/88130 [======================>.......] - ETA: 41s - loss: 1.6194 - acc: 0.5163"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for iteration in range(1):\n",
    "    model.fit(X, y, batch_size=128, epochs=50) ############ 50 epochs\n",
    "\n",
    "    for temperature in [1.5, 0.75, 0.25]:\n",
    "        print()\n",
    "        print('----- temperature parameter:', temperature)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = \"shall i compare thee to a summer's day?\\n\"\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        line_count = 0\n",
    "        while (line_count <= 12):\n",
    "            x = np.zeros((1, length, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x[0, t, char_index_map[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_char = index_char_map[next_index]\n",
    "            \n",
    "            if next_char == '\\n':\n",
    "                line_count += 1\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Epoch 1/50\n",
    "88130/88130 [==============================] - 51s 573us/step - loss: 1.7970 - acc: 0.4644\n",
    "Epoch 2/50\n",
    "88130/88130 [==============================] - 33s 369us/step - loss: 1.6771 - acc: 0.4935\n",
    "Epoch 3/50\n",
    "88130/88130 [==============================] - 43s 484us/step - loss: 1.6178 - acc: 0.5080\n",
    "Epoch 4/50\n",
    "88130/88130 [==============================] - 61s 694us/step - loss: 1.5742 - acc: 0.5187\n",
    "Epoch 5/50\n",
    "88130/88130 [==============================] - 55s 624us/step - loss: 1.5492 - acc: 0.5241\n",
    "Epoch 6/50\n",
    "88130/88130 [==============================] - 56s 635us/step - loss: 1.5205 - acc: 0.5332\n",
    "Epoch 7/50\n",
    "88130/88130 [==============================] - 61s 696us/step - loss: 1.5013 - acc: 0.5371\n",
    "Epoch 8/50\n",
    "88130/88130 [==============================] - 61s 690us/step - loss: 1.4870 - acc: 0.5431\n",
    "Epoch 9/50\n",
    "88130/88130 [==============================] - 46s 519us/step - loss: 1.4697 - acc: 0.5464\n",
    "Epoch 10/50\n",
    "88130/88130 [==============================] - 51s 574us/step - loss: 1.4608 - acc: 0.54881s - \n",
    "Epoch 11/50\n",
    "88130/88130 [==============================] - 46s 527us/step - loss: 1.4532 - acc: 0.5496\n",
    "Epoch 12/50\n",
    "88130/88130 [==============================] - 48s 544us/step - loss: 1.4473 - acc: 0.5517\n",
    "Epoch 13/50\n",
    "88130/88130 [==============================] - 51s 583us/step - loss: 1.4431 - acc: 0.5526\n",
    "Epoch 14/50\n",
    "88130/88130 [==============================] - 54s 615us/step - loss: 1.4380 - acc: 0.5540\n",
    "Epoch 15/50\n",
    "88130/88130 [==============================] - 59s 672us/step - loss: 1.4388 - acc: 0.5534\n",
    "Epoch 16/50\n",
    "88130/88130 [==============================] - 60s 680us/step - loss: 1.4402 - acc: 0.5535\n",
    "Epoch 17/50\n",
    "88130/88130 [==============================] - 64s 730us/step - loss: 1.4389 - acc: 0.55206s - loss: 1.4326 - acc - ETA: 5s - ETA: 3s - l - ETA: 0s - loss: 1.4381 - a\n",
    "Epoch 18/50\n",
    "88130/88130 [==============================] - 69s 779us/step - loss: 1.4424 - acc: 0.5545\n",
    "Epoch 19/50\n",
    "88130/88130 [==============================] - 72s 821us/step - loss: 1.4461 - acc: 0.5529\n",
    "Epoch 20/50\n",
    "88130/88130 [==============================] - 75s 853us/step - loss: 1.4475 - acc: 0.5515\n",
    "Epoch 21/50\n",
    "88130/88130 [==============================] - 78s 886us/step - loss: 1.4551 - acc: 0.5500\n",
    "Epoch 22/50\n",
    "88130/88130 [==============================] - 80s 910us/step - loss: 1.4483 - acc: 0.5510\n",
    "Epoch 23/50\n",
    "88130/88130 [==============================] - 83s 938us/step - loss: 1.4501 - acc: 0.5537\n",
    "Epoch 24/50\n",
    "88130/88130 [==============================] - 86s 971us/step - loss: 1.4599 - acc: 0.5492\n",
    "Epoch 25/50\n",
    "88130/88130 [==============================] - 89s 1ms/step - loss: 1.4617 - acc: 0.5495\n",
    "Epoch 26/50\n",
    "88130/88130 [==============================] - 92s 1ms/step - loss: 1.4742 - acc: 0.5475\n",
    "Epoch 27/50\n",
    "88130/88130 [==============================] - 95s 1ms/step - loss: 1.4748 - acc: 0.5486\n",
    "Epoch 28/50\n",
    "88130/88130 [==============================] - 99s 1ms/step - loss: 1.4760 - acc: 0.5477\n",
    "Epoch 29/50\n",
    "88130/88130 [==============================] - 101s 1ms/step - loss: 1.4866 - acc: 0.5422\n",
    "Epoch 30/50\n",
    "88130/88130 [==============================] - 107s 1ms/step - loss: 1.4939 - acc: 0.5429\n",
    "Epoch 31/50\n",
    "88130/88130 [==============================] - 107s 1ms/step - loss: 1.5019 - acc: 0.5418\n",
    "Epoch 32/50\n",
    "88130/88130 [==============================] - 111s 1ms/step - loss: 1.5027 - acc: 0.5407\n",
    "Epoch 33/50\n",
    "88130/88130 [==============================] - 115s 1ms/step - loss: 1.5082 - acc: 0.5392\n",
    "Epoch 34/50\n",
    "88130/88130 [==============================] - 118s 1ms/step - loss: 1.5045 - acc: 0.5412\n",
    "Epoch 35/50\n",
    "88130/88130 [==============================] - 123s 1ms/step - loss: 1.5203 - acc: 0.5377 0s - loss: 1.5202 - acc: 0.537\n",
    "Epoch 36/50\n",
    "88130/88130 [==============================] - 128s 1ms/step - loss: 1.5202 - acc: 0.5355\n",
    "Epoch 37/50\n",
    "88130/88130 [==============================] - 131s 1ms/step - loss: 1.5253 - acc: 0.5362 12s - los - ETA: 2s - loss: 1.5238 \n",
    "Epoch 38/50\n",
    "88130/88130 [==============================] - 133s 2ms/step - loss: 1.5366 - acc: 0.5331A: 2:04 - loss: - ET - ETA: 0s - loss: 1.5361 - acc: 0\n",
    "Epoch 39/50\n",
    "88130/88130 [==============================] - 136s 2ms/step - loss: 1.5311 - acc: 0.5348\n",
    "Epoch 40/50\n",
    "88130/88130 [==============================] - 139s 2ms/step - loss: 1.5399 - acc: 0.5338\n",
    "Epoch 41/50\n",
    "88130/88130 [==============================] - 144s 2ms/step - loss: 1.5524 - acc: 0.5317\n",
    "Epoch 42/50\n",
    "88130/88130 [==============================] - 146s 2ms/step - loss: 1.5625 - acc: 0.5285\n",
    "Epoch 43/50\n",
    "88130/88130 [==============================] - 149s 2ms/step - loss: 1.5628 - acc: 0.5263\n",
    "Epoch 44/50\n",
    "88130/88130 [==============================] - 153s 2ms/step - loss: 1.5592 - acc: 0.5289\n",
    "Epoch 45/50\n",
    "88130/88130 [==============================] - 156s 2ms/step - loss: 1.5581 - acc: 0.5290\n",
    "Epoch 46/50\n",
    "88130/88130 [==============================] - 161s 2ms/step - loss: 1.5783 - acc: 0.5234\n",
    "Epoch 47/50\n",
    "88130/88130 [==============================] - 165s 2ms/step - loss: 1.5790 - acc: 0.5235\n",
    "Epoch 48/50\n",
    "88130/88130 [==============================] - 167s 2ms/step - loss: 1.5707 - acc: 0.5254\n",
    "Epoch 49/50\n",
    "88130/88130 [==============================] - 169s 2ms/step - loss: 1.5808 - acc: 0.5243\n",
    "Epoch 50/50\n",
    "88130/88130 [==============================] - 173s 2ms/step - loss: 1.5876 - acc: 0.5220\n",
    "\n",
    "----- temperature parameter: 1.5\n",
    "----- Generating with seed: \"shall i compare thee to a summer's day?\n",
    "\"\n",
    "shall i compare thee to a summer's day?\n",
    "disired doth loten night afterwired?\n",
    "  loot is a full eyed, ors apvinved gaare.\n",
    "when thrust altiming eare dwelll's opers am ofunliwo,\n",
    "be mistage weere's steetlciore flowers comose,\n",
    "or aletk knows thee brasted heoflery unoth.\n",
    "o thanrany, it mogchs sways by tonel.\n",
    "and give anture motmeint hims of fers placrine's,\n",
    "alonging not tack goes threwn hath far time)\n",
    "not rered jewel prisont leave to avermf aummer\n",
    "edtr'i hrow nane sor five's ead,\n",
    "using gained anule conben noom she curi,\n",
    "numing time sliaves in stees whit inwaliarly,\n",
    "deov'nsetfcl my e'ey eyide with in thee,\n",
    "\n",
    "\n",
    "----- temperature parameter: 0.75\n",
    "----- Generating with seed: \"shall i compare thee to a summer's day?\n",
    "\"\n",
    "shall i compare thee to a summer's day?\n",
    "  wild far to steal at be no blering beaut.\n",
    "  but to me a woettuen dwill, and so founts\n",
    "sumaect the might and again to less in that is shade,\n",
    "the that lawcing with on some deade mine,\n",
    "but shall to be are even my gone,\n",
    "and by yourse is is sad pity,\n",
    "my heart truths shadow my fair faili aj in do brojenf for besiker,\n",
    "a joutiin thy hath in his his mad, staig,\n",
    "being the would worths stop at not my self,\n",
    "  and starign too all should bart the bean,\n",
    "and therefore gracinaeck of end, own with eecw part.\n",
    "to praise your do and a lome's and gracit,\n",
    "and then age' de lies in the for thee but row,\n",
    "\n",
    "\n",
    "----- temperature parameter: 0.25\n",
    "----- Generating with seed: \"shall i compare thee to a summer's day?\n",
    "\"\n",
    "shall i compare thee to a summer's day?\n",
    "and this not the seemed and the beauty of thee,\n",
    "and thou my self this confeered to me,\n",
    "to me a loss thou my all to parts and thought,\n",
    "and this in thee art my thee to the seemed:\n",
    "then i shame hath with the seeming thy seee wind,\n",
    "and thou my self shall to parts, and and of thee,\n",
    "and and my self thou art stand to my self,\n",
    "and thou my self away, and stand to,\n",
    "that i sea it me thee to the worth to be than my love,\n",
    "  then my love and thou my love and thee,\n",
    "and thou my self though the seemen to steal to cannot to thee,\n",
    "that i am shall in thee to see are to with a come,\n",
    "and and thou my love and that to show,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
