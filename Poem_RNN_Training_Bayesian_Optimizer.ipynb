{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "\n",
    "from functools import partial\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of poems: 154\n"
     ]
    }
   ],
   "source": [
    "text = open(os.path.join(os.getcwd(), 'data/shakespeare.txt')).read().lower().split('\\n')\n",
    "poem_list = []\n",
    "raw_text = ''\n",
    "for j in range(len(text) + 1):\n",
    "    if j == len(text):\n",
    "        poem_list.append(raw_text)\n",
    "    elif text[j] == '':\n",
    "        if raw_text != '':\n",
    "            poem_list.append(raw_text)\n",
    "        raw_text = ''\n",
    "        continue\n",
    "    elif text[j][-1].isdigit():\n",
    "        continue\n",
    "    else:\n",
    "        subsentence = text[j] + '\\n'\n",
    "        raw_text += subsentence  \n",
    "print('Number of poems:', len(poem_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function for creating sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize into sequences of characters\n",
    "def create_sequence(raw_text, length, step):\n",
    "    sequences = []\n",
    "    next_chars = []\n",
    "    for i in range(0, len(raw_text) - length, step):\n",
    "        # select sequence of tokens\n",
    "        seq = raw_text[i:i + length]\n",
    "        # store\n",
    "        sequences.append(seq)\n",
    "        next_chars.append(raw_text[i + length])\n",
    "    return sequences, next_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 88130\n"
     ]
    }
   ],
   "source": [
    "length = 40\n",
    "step = 1\n",
    "sequences = []\n",
    "next_chars = []\n",
    "for poem in poem_list:\n",
    "    sub_sequences, sub_next_chars = create_sequence(poem, length, step)\n",
    "    sequences += sub_sequences\n",
    "    next_chars += sub_next_chars\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Mappings and Inverse Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 38\n"
     ]
    }
   ],
   "source": [
    "poem_string = \"\".join(poem_list)\n",
    "chars = sorted(list(set(poem_string)))\n",
    "char_index_map = dict((c, i) for i, c in enumerate(chars))\n",
    "index_char_map = dict((i, c) for i, c in enumerate(chars))\n",
    "vocab_size = len(char_index_map)\n",
    "print('Vocabulary Size: %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(sequences), length, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sequences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_index_map[char]] = 1\n",
    "    y[i, char_index_map[next_chars[i]]] = 1    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for building and fitting RNN Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(LSTM_size, dropout_rate):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(LSTM_size, input_shape=(length, len(chars))))\n",
    "    model.add(Dense(len(chars)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Activation('softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(LSTM_size, verbose, dropout_rate, lr):\n",
    "\n",
    "    # Create the model using a specified hyperparameters.\n",
    "    model = build_model(LSTM_size, dropout_rate)\n",
    "\n",
    "    # Train the model for a specified number of epochs.\n",
    "    optimizer = RMSprop(lr)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    # Train the model with the train dataset.\n",
    "    model.fit(X, y, batch_size=128, epochs=10, verbose=verbose)\n",
    "\n",
    "    # Evaluate the model with the eval dataset.\n",
    "    score = model.evaluate(X, y, verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "\n",
    "    # Return the accuracy.\n",
    "\n",
    "    return score[1]\n",
    "\n",
    "verbose = 1\n",
    "LSTM_size = 128\n",
    "fit_with_partial = partial(fit_model, LSTM_size, verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimization (tuning hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | dropou... |    lr     |\n",
      "-------------------------------------------------\n",
      "WARNING:tensorflow:From C:\\Users\\Steve\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Steve\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Steve\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Steve\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Steve\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\Steve\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Steve\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Steve\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\Steve\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Steve\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Steve\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\Steve\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Steve\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Steve\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Steve\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Steve\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "88130/88130 [==============================] - 52s 596us/step - loss: 2.5321 - acc: 0.3069\n",
      "Epoch 2/10\n",
      "88130/88130 [==============================] - 52s 592us/step - loss: 2.2037 - acc: 0.3887\n",
      "Epoch 3/10\n",
      "88130/88130 [==============================] - 52s 596us/step - loss: 2.1136 - acc: 0.4129\n",
      "Epoch 4/10\n",
      "88130/88130 [==============================] - 55s 624us/step - loss: 2.0516 - acc: 0.4327\n",
      "Epoch 5/10\n",
      "88130/88130 [==============================] - 58s 662us/step - loss: 2.0092 - acc: 0.4432\n",
      "Epoch 6/10\n",
      "88130/88130 [==============================] - 61s 689us/step - loss: 1.9751 - acc: 0.4531\n",
      "Epoch 7/10\n",
      "88130/88130 [==============================] - 61s 698us/step - loss: 1.9398 - acc: 0.4621\n",
      "Epoch 8/10\n",
      "88130/88130 [==============================] - 70s 789us/step - loss: 1.9273 - acc: 0.4630\n",
      "Epoch 9/10\n",
      "88130/88130 [==============================] - 93s 1ms/step - loss: 1.9019 - acc: 0.4699\n",
      "Epoch 10/10\n",
      "88130/88130 [==============================] - 79s 892us/step - loss: 1.8885 - acc: 0.47510s - loss: 1.8878 - acc: 0\n",
      "Test loss: 1.3356850396054267\n",
      "Test accuracy: 0.5981050720526495\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.5981  \u001b[0m | \u001b[0m 0.2668  \u001b[0m | \u001b[0m 0.007231\u001b[0m |\n",
      "Epoch 1/10\n",
      "88130/88130 [==============================] - 80s 906us/step - loss: 2.4127 - acc: 0.3198\n",
      "Epoch 2/10\n",
      "88130/88130 [==============================] - 83s 944us/step - loss: 2.0331 - acc: 0.4113\n",
      "Epoch 3/10\n",
      "88130/88130 [==============================] - 89s 1ms/step - loss: 1.8943 - acc: 0.4459\n",
      "Epoch 4/10\n",
      "88130/88130 [==============================] - 113s 1ms/step - loss: 1.8139 - acc: 0.4678\n",
      "Epoch 5/10\n",
      "88130/88130 [==============================] - 102s 1ms/step - loss: 1.7538 - acc: 0.4836\n",
      "Epoch 6/10\n",
      "88130/88130 [==============================] - 114s 1ms/step - loss: 1.7041 - acc: 0.4985\n",
      "Epoch 7/10\n",
      "88130/88130 [==============================] - 105s 1ms/step - loss: 1.6640 - acc: 0.5077 2s - loss: 1.6\n",
      "Epoch 8/10\n",
      "88130/88130 [==============================] - 134s 2ms/step - loss: 1.6280 - acc: 0.5170\n",
      "Epoch 9/10\n",
      "88130/88130 [==============================] - 114s 1ms/step - loss: 1.5891 - acc: 0.5282\n",
      "Epoch 10/10\n",
      "88130/88130 [==============================] - 120s 1ms/step - loss: 1.5590 - acc: 0.5357\n",
      "Test loss: 1.3119721049241468\n",
      "Test accuracy: 0.5945875411324181\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.5946  \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.003093\u001b[0m |\n",
      "Epoch 1/10\n",
      "88130/88130 [==============================] - 144s 2ms/step - loss: 2.7441 - acc: 0.2471 0s - loss: 2.7460 - acc: 0\n",
      "Epoch 2/10\n",
      "88130/88130 [==============================] - 151s 2ms/step - loss: 2.3904 - acc: 0.3303\n",
      "Epoch 3/10\n",
      "88130/88130 [==============================] - 154s 2ms/step - loss: 2.2614 - acc: 0.3623 2s - loss: 2.2613 - \n",
      "Epoch 4/10\n",
      "88130/88130 [==============================] - 163s 2ms/step - loss: 2.1648 - acc: 0.3858\n",
      "Epoch 5/10\n",
      "88130/88130 [==============================] - 179s 2ms/step - loss: 2.1069 - acc: 0.4005\n",
      "Epoch 6/10\n",
      "88130/88130 [==============================] - 189s 2ms/step - loss: 2.0496 - acc: 0.4149\n",
      "Epoch 7/10\n",
      "88130/88130 [==============================] - 189s 2ms/step - loss: 2.0059 - acc: 0.4263\n",
      "Epoch 8/10\n",
      "88130/88130 [==============================] - 197s 2ms/step - loss: 1.9729 - acc: 0.4336\n",
      "Epoch 9/10\n",
      "88130/88130 [==============================] - 222s 3ms/step - loss: 1.9390 - acc: 0.4428\n",
      "Epoch 10/10\n",
      "88130/88130 [==============================] - 216s 2ms/step - loss: 1.9111 - acc: 0.4501\n",
      "Test loss: 1.6163454806007944\n",
      "Test accuracy: 0.5131056393963463\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.5131  \u001b[0m | \u001b[0m 0.1587  \u001b[0m | \u001b[0m 0.001014\u001b[0m |\n",
      "Epoch 1/10\n",
      "88130/88130 [==============================] - 204s 2ms/step - loss: 2.4993 - acc: 0.3026\n",
      "Epoch 2/10\n",
      "88130/88130 [==============================] - 200s 2ms/step - loss: 2.1350 - acc: 0.3942\n",
      "Epoch 3/10\n",
      "88130/88130 [==============================] - 211s 2ms/step - loss: 2.0091 - acc: 0.4255\n",
      "Epoch 4/10\n",
      "88130/88130 [==============================] - 213s 2ms/step - loss: 1.9317 - acc: 0.4468\n",
      "Epoch 5/10\n",
      "88130/88130 [==============================] - 226s 3ms/step - loss: 1.8831 - acc: 0.4598\n",
      "Epoch 6/10\n",
      "88130/88130 [==============================] - 226s 3ms/step - loss: 1.8317 - acc: 0.4730\n",
      "Epoch 7/10\n",
      "88130/88130 [==============================] - 238s 3ms/step - loss: 1.7929 - acc: 0.4854\n",
      "Epoch 8/10\n",
      "88130/88130 [==============================] - 248s 3ms/step - loss: 1.7642 - acc: 0.4910\n",
      "Epoch 9/10\n",
      "88130/88130 [==============================] - 261s 3ms/step - loss: 1.7400 - acc: 0.4979\n",
      "Epoch 10/10\n",
      "88130/88130 [==============================] - 271s 3ms/step - loss: 1.7115 - acc: 0.5057\n",
      "Test loss: 1.3197922034349099\n",
      "Test accuracy: 0.5970498127765801\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.597   \u001b[0m | \u001b[0m 0.1745  \u001b[0m | \u001b[0m 0.003521\u001b[0m |\n",
      "Epoch 1/10\n",
      "88130/88130 [==============================] - 287s 3ms/step - loss: 2.5507 - acc: 0.3010\n",
      "Epoch 2/10\n",
      "88130/88130 [==============================] - 264s 3ms/step - loss: 2.2227 - acc: 0.3814\n",
      "Epoch 3/10\n",
      "88130/88130 [==============================] - 277s 3ms/step - loss: 2.1240 - acc: 0.4084\n",
      "Epoch 4/10\n",
      "88130/88130 [==============================] - 310s 4ms/step - loss: 2.0565 - acc: 0.4264\n",
      "Epoch 5/10\n",
      "88130/88130 [==============================] - 323s 4ms/step - loss: 2.0072 - acc: 0.4422\n",
      "Epoch 6/10\n",
      "88130/88130 [==============================] - 309s 4ms/step - loss: 1.9737 - acc: 0.4506\n",
      "Epoch 7/10\n",
      "88130/88130 [==============================] - 339s 4ms/step - loss: 1.9381 - acc: 0.4573\n",
      "Epoch 8/10\n",
      "88130/88130 [==============================] - 327s 4ms/step - loss: 1.9071 - acc: 0.4674\n",
      "Epoch 9/10\n",
      "88130/88130 [==============================] - 328s 4ms/step - loss: 1.8898 - acc: 0.4708\n",
      "Epoch 10/10\n",
      "88130/88130 [==============================] - 326s 4ms/step - loss: 1.8681 - acc: 0.4773\n",
      "Test loss: 1.3368320442052213\n",
      "Test accuracy: 0.59419040054465\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.5942  \u001b[0m | \u001b[0m 0.2587  \u001b[0m | \u001b[0m 0.005434\u001b[0m |\n",
      "Epoch 1/10\n",
      "88130/88130 [==============================] - 418s 5ms/step - loss: 2.5667 - acc: 0.2974\n",
      "Epoch 2/10\n",
      "88130/88130 [==============================] - 398s 5ms/step - loss: 2.2466 - acc: 0.3801\n",
      "Epoch 3/10\n",
      "88130/88130 [==============================] - 402s 5ms/step - loss: 2.1411 - acc: 0.4091\n",
      "Epoch 4/10\n",
      "88130/88130 [==============================] - 408s 5ms/step - loss: 2.0667 - acc: 0.4261\n",
      "Epoch 5/10\n",
      "88130/88130 [==============================] - 425s 5ms/step - loss: 2.0289 - acc: 0.4362\n",
      "Epoch 6/10\n",
      "88130/88130 [==============================] - 441s 5ms/step - loss: 1.9982 - acc: 0.4455\n",
      "Epoch 7/10\n",
      "88130/88130 [==============================] - 463s 5ms/step - loss: 1.9650 - acc: 0.4546\n",
      "Epoch 8/10\n",
      "88130/88130 [==============================] - 471s 5ms/step - loss: 1.9390 - acc: 0.4593\n",
      "Epoch 9/10\n",
      "88130/88130 [==============================] - 467s 5ms/step - loss: 1.9232 - acc: 0.4654\n",
      "Epoch 10/10\n",
      "88130/88130 [==============================] - 464s 5ms/step - loss: 1.9060 - acc: 0.4672\n",
      "Test loss: 1.3460372564890835\n",
      "Test accuracy: 0.5889594916600477\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.589   \u001b[0m | \u001b[0m 0.2677  \u001b[0m | \u001b[0m 0.006884\u001b[0m |\n",
      "Epoch 1/10\n",
      "88130/88130 [==============================] - 456s 5ms/step - loss: 2.4131 - acc: 0.3259\n",
      "Epoch 2/10\n",
      "88130/88130 [==============================] - 431s 5ms/step - loss: 2.0588 - acc: 0.4133\n",
      "Epoch 3/10\n",
      "88130/88130 [==============================] - 464s 5ms/step - loss: 1.9484 - acc: 0.4436\n",
      "Epoch 4/10\n",
      "88130/88130 [==============================] - 451s 5ms/step - loss: 1.8971 - acc: 0.4595\n",
      "Epoch 5/10\n",
      "88130/88130 [==============================] - 485s 6ms/step - loss: 1.8503 - acc: 0.4714\n",
      "Epoch 6/10\n",
      "88130/88130 [==============================] - 518s 6ms/step - loss: 1.8154 - acc: 0.4796\n",
      "Epoch 7/10\n",
      "88130/88130 [==============================] - 521s 6ms/step - loss: 1.7899 - acc: 0.4864\n",
      "Epoch 8/10\n",
      "88130/88130 [==============================] - 498s 6ms/step - loss: 1.7701 - acc: 0.4912\n",
      "Epoch 9/10\n",
      "88130/88130 [==============================] - 532s 6ms/step - loss: 1.7529 - acc: 0.4947\n",
      "Epoch 10/10\n",
      "88130/88130 [==============================] - 511s 6ms/step - loss: 1.7396 - acc: 0.4994\n",
      "Test loss: 1.337634353613077\n",
      "Test accuracy: 0.5849086576648134\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.5849  \u001b[0m | \u001b[0m 0.1818  \u001b[0m | \u001b[0m 0.008793\u001b[0m |\n",
      "Epoch 1/10\n",
      "88130/88130 [==============================] - 579s 7ms/step - loss: 2.2779 - acc: 0.3526\n",
      "Epoch 2/10\n",
      "88130/88130 [==============================] - 538s 6ms/step - loss: 1.9163 - acc: 0.4417\n",
      "Epoch 3/10\n",
      "88130/88130 [==============================] - 534s 6ms/step - loss: 1.8064 - acc: 0.4710\n",
      "Epoch 4/10\n",
      "88130/88130 [==============================] - 551s 6ms/step - loss: 1.7281 - acc: 0.4925\n",
      "Epoch 5/10\n",
      "88130/88130 [==============================] - 551s 6ms/step - loss: 1.6764 - acc: 0.5048\n",
      "Epoch 6/10\n",
      "88130/88130 [==============================] - 583s 7ms/step - loss: 1.6320 - acc: 0.5179\n",
      "Epoch 7/10\n",
      "88130/88130 [==============================] - 559s 6ms/step - loss: 1.6076 - acc: 0.5246\n",
      "Epoch 8/10\n",
      "88130/88130 [==============================] - 586s 7ms/step - loss: 1.5785 - acc: 0.5314\n",
      "Epoch 9/10\n",
      "88130/88130 [==============================] - 590s 7ms/step - loss: 1.5529 - acc: 0.5398\n",
      "Epoch 10/10\n",
      "88130/88130 [==============================] - 602s 7ms/step - loss: 1.5330 - acc: 0.5453\n",
      "Test loss: 1.253393336779195\n",
      "Test accuracy: 0.6115851582888914\n",
      "| \u001b[95m 8       \u001b[0m | \u001b[95m 0.6116  \u001b[0m | \u001b[95m 0.111   \u001b[0m | \u001b[95m 0.006738\u001b[0m |\n",
      "Epoch 1/10\n",
      "88130/88130 [==============================] - 652s 7ms/step - loss: 2.5864 - acc: 0.2941\n",
      "Epoch 2/10\n",
      "88130/88130 [==============================] - 587s 7ms/step - loss: 2.2545 - acc: 0.3752\n",
      "Epoch 3/10\n",
      "88130/88130 [==============================] - 596s 7ms/step - loss: 2.1466 - acc: 0.4036\n",
      "Epoch 4/10\n",
      "88130/88130 [==============================] - 648s 7ms/step - loss: 2.0865 - acc: 0.4210\n",
      "Epoch 5/10\n",
      "88130/88130 [==============================] - 609s 7ms/step - loss: 2.0265 - acc: 0.4369\n",
      "Epoch 6/10\n",
      "88130/88130 [==============================] - 645s 7ms/step - loss: 1.9962 - acc: 0.4461\n",
      "Epoch 7/10\n",
      "88130/88130 [==============================] - 686s 8ms/step - loss: 1.9664 - acc: 0.4545\n",
      "Epoch 8/10\n",
      "88130/88130 [==============================] - 660s 7ms/step - loss: 1.9365 - acc: 0.4594\n",
      "Epoch 9/10\n",
      "88130/88130 [==============================] - 674s 8ms/step - loss: 1.9153 - acc: 0.4658\n",
      "Epoch 10/10\n",
      "88130/88130 [==============================] - 709s 8ms/step - loss: 1.9034 - acc: 0.4683\n",
      "Test loss: 1.354055775367049\n",
      "Test accuracy: 0.5944059911494384\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.5944  \u001b[0m | \u001b[0m 0.2669  \u001b[0m | \u001b[0m 0.005631\u001b[0m |\n",
      "Epoch 1/10\n",
      "88130/88130 [==============================] - 761s 9ms/step - loss: 2.5790 - acc: 0.2840\n",
      "Epoch 2/10\n",
      "88130/88130 [==============================] - 681s 8ms/step - loss: 2.2224 - acc: 0.3689\n",
      "Epoch 3/10\n",
      "88130/88130 [==============================] - 710s 8ms/step - loss: 2.0711 - acc: 0.4083\n",
      "Epoch 4/10\n",
      "88130/88130 [==============================] - 706s 8ms/step - loss: 1.9985 - acc: 0.4243\n",
      "Epoch 5/10\n",
      "88130/88130 [==============================] - 713s 8ms/step - loss: 1.9390 - acc: 0.4421\n",
      "Epoch 6/10\n",
      "88130/88130 [==============================] - 701s 8ms/step - loss: 1.8920 - acc: 0.4530\n",
      "Epoch 7/10\n",
      "88130/88130 [==============================] - 743s 8ms/step - loss: 1.8499 - acc: 0.4661\n",
      "Epoch 8/10\n",
      "88130/88130 [==============================] - 773s 9ms/step - loss: 1.8173 - acc: 0.4742\n",
      "Epoch 9/10\n",
      "11264/88130 [==>...........................] - ETA: 11:14 - loss: 1.7529 - acc: 0.4890"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Bounded region of parameter space\n",
    "pbounds = {'dropout_rate': (0.1, 0.5), 'lr': (1e-4, 1e-2)}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=fit_with_partial,\n",
    "    pbounds=pbounds,\n",
    "    verbose=2,  # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "optimizer.maximize(init_points=10, n_iter=10,)\n",
    "\n",
    "for i, res in enumerate(optimizer.res):\n",
    "    print(\"Iteration {}: \\n\\t{}\".format(i, res))\n",
    "\n",
    "print(optimizer.max)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
